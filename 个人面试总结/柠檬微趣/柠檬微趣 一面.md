### 总结
全是基础知识，没有问项目相关内容。
较深知识点的熟练度欠佳。涉及算法方面内容需要加强。


### 详细记录
- 数组与链表
	- 含义
	- 区别
	- 优缺点
- 排序
	- 堆排序
		- 不额外使用空间的堆排序如何实现
	- 快速排序
		- 时间复杂度
		- 什么情况会达到最差复杂度，最差复杂度是多少
		- 遇到相同元素能否排序正确，如何优化相同元素
- HDFS
	- 写数据流程
		- 写过程中，单个 DN 宕机后的处理
		- 如何保证数据块完整性
		- DN 机器恢复后的处理
	- 读数据流程
		- 是否会读取到不完整的块
		- Client 如何保证读取到完整块
	- NameNode 与 DataNode 交互情况
- 多线程：多线程操作一个对象，如何保证安全性
	- ThreadLocal（对象不需要共享的情况下）
	- `synchronized`、`ReentrantLock`（悲观锁实现）
	- `atomicInteger` 等 `Atomic` 类（乐观锁实现）
		- CAS 算法原理，底层如何实现的 CAS 原子性
- 如何在一个 3 G 物理机下部署两个 2 G 的 Java 服务
	- 虚拟内存
	- 外部监控


### 问题复盘
#### 不额外使用空间的堆排序如何实现
1. 构建堆：首先将数组构建成一个大根堆（或小根堆，取决于要实现升序还是降序排序）。从数组的中间位置开始，依次对每个非叶子节点进行调整，使其满足堆的性质（即父节点的值大于（或小于）子节点的值）。这一步的时间复杂度为 $O(n)$，其中 $n$ 是数组的长度。
2. 排序：
	1. 交换堆顶元素和堆的最后一个元素。此时，堆的最后一个元素就是已排序的最大值（或最小值）。
	2. 由于交换后可能破坏堆的性质，需要对堆顶元素进行调整，使其重新满足堆的性质。调整的过程与构建堆时类似，但规模逐渐减小。
	3. 重复上述步骤，每次将未排序部分的最大值（或最小值）交换到正确位置，直到整个数组排序完成。

堆排序的时间复杂度为 $O(nlogn)$，其中 $n$ 是待排序元素的数量。它具有较好的性能，并且不需要额外的存储空间，但堆排序不是稳定的排序算法，相同元素的相对顺序可能会在排序过程中发生改变。

#### 快速排序什么情况会达到最差复杂度，最差复杂度是多少
当每次选择的基准元素都是当前子序列中的最小（或最大）元素时，快速排序会退化为冒泡排序，从而导致最差的时间复杂度。也就是说，每次划分只能将待排序序列减少一个元素，这种极端的情况会使得快速排序的时间复杂度最差。

快速排序最差的时间复杂度为 $O(n^2)$。

例如，对于已经有序的序列，如 `{1, 2, 3, 4, 5}` ，如果总是选择第一个元素作为基准，那么每次划分只能确定一个元素的位置，需要经过 $n - 1$ 次划分才能完成排序，总的时间复杂度就为 $O(n^2)$。

为了避免这种最坏情况的发生，可以采用随机选择基准、三数取中等优化策略，使得快速排序在大多数情况下都能保持较好的性能，其平均时间复杂度为 $O(nlogn)$。

#### 快速排序遇到相同元素能否排序正确，如何优化相同元素的快速排序
能够正确排序。

**三相切割快速排序（Three-way Partitioning Quick Sort）**：传统的快速排序是二路切割，将数组分为比基准大或小于等于基准的两部分。而三相切割则增加了一个等于基准的部分。通过使用额外的指针来交换与基准相等的元素，并在递归时只对小于和大于基准的子序列进行排序，可以有效地减少子问题的大小，因为等于基准的那一段不需要再进行排序。

快速排序一般还会优化 **基准选择**：
1. **三数取中法选择基准**：基本思想是对待排序序列中 low、mid、high 三个位置上的数据进行排序，取它们中间的那个数据作为枢轴。例如，对于序列 `arr`，取 `mid = low + ((high - low) >> 1)`，即中间元素的下标，然后通过比较和交换，使得 `arr[mid] <= arr[low] <= arr[high]`，最后返回 `arr[low]` 作为枢轴。这种方法可以缓解在待排序序列部分有序时，固定选取枢轴导致效率低下的问题，减少快排大约 14%的比较次数。一般还可以扩展到从五个或更多元素中选取中间值作为枢轴。
2. **结合插入排序**：当待排序序列的长度分割到一定大小后（例如长度 n = 10），使用插入排序。因为对于很小和部分有序的数组，快排的效率可能不如插入排序。
3. **随机取基准法**：通过随机数生成的方式在待排序列中随机选取一个元素作为基准，而不是固定选择第一个或最后一个元素。这降低了出现最坏情况（每次划分只能使待排序序列减一）的概率，在一定程度上提高了效率。

#### HDFS 写过程中，单个 DN 宕机后的处理
1. **关闭 pipeline**：HDFS 会关闭当前的 pipeline，以防止数据继续写入到宕机的 DataNode 中。
2. **记录未完成的块信息**：NameNode 会记录宕机的 DataNode 中未完成的块信息，这些信息将用于后续的数据恢复。
3. **重新建立 pipeline**：HDFS 会在剩余的正常 DataNode 中重新建立 pipeline，以继续写入数据。
4. **数据恢复**：在新的 pipeline 建立完成后，HDFS 会将未完成的块数据重新写入到新的 DataNode 中，以保证数据的完整性。
5. **副本管理**：HDFS 会确保数据的副本数满足预设的要求。如果宕机的 DataNode 中的数据副本丢失，HDFS 会在其他 DataNode 中创建新的副本，以保证数据的可用性和容错性。

在数据快要写完时，pipeline 上最后一个 DataNode 宕机了，前两个 DataNode 中已经写入的数据不会被丢弃，而是会被保留下来，并在后续的数据恢复过程中继续使用。HDFS 会通过上述的数据恢复机制，将未完成的块数据重新写入到新的 DataNode 中，以保证数据的完整性。同时，HDFS 也会确保数据的副本数满足预设的要求，以提高数据的可用性和容错性。

**实际场景：**
假如我现在写入一个文件到HDFS，NameNode给我分配了dn1、dn2、dn3三个DataNode，pipeline的结构为 client->dn1->dn2->dn3，这个文件分为了10个block。现在我已经写完了9个block，每个block都得到了确认，现在第10个block在写入过程中，dn3宕机了。此时会关闭pipeline，记录未完成的这个块block10，然后会在剩余的DataNode中重新建立pipeline，此时剩余DataNode是会排除dn1、dn2、dn3三个节点。在新建立的pipeline中将block10重新进行上传。我疑惑两个问题： 1）重建pipeline时从剩余DataNode选，这时会排除dn1、dn2、dn3三个节点吗？ 2）是会直接从block10开始上传，原本上传完毕的block1~block9不会再次上传吗？

当 pipeline 中的 dn3 宕机时，HDFS 确实会关闭 pipeline 并记录未完成的块（block10）。然后，它会在剩余的 DataNode 中重新建立 pipeline。

但是，在重建 pipeline 时，不会排除 dn1 和 dn2。实际上，dn1 和 dn2 中已经写入的数据仍然是有效的，并且会被保留继续使用。新建立的 pipeline 可能是 client->dn1->dn2 或者 client->dn2（具体取决于 HDFS 的实现和策略），然后在这个新的 pipeline 中继续上传 block10。原本上传完毕的 block1~block9 不会再次上传。

HDFS 会确保数据的完整性和一致性。在新的 pipeline 建立完成后，只会上传未完成的 block10。同时，NameNode 会知道这个 block 处于未完成状态（under-replicated），后续它会安排新的副本以达到要求的备份数。

这样的机制保证了在 DataNode 宕机的情况下，数据的写入能够尽可能地继续进行，同时避免了已经成功写入的数据被重复上传，提高了数据写入的效率和可靠性。

#### HDFS Client 如何保证读取到完整块
主要是通过检验和来确保完整性，NameNode 确实知道每个 block 的正确检验和。每个 block 都有对应的 block meta 文件，meta 文件中的主要内容为对应 block 文件数据的校验和，用以验证 block 文件的完整性；其他的元数据信息，例如生成时间等。

在写入时，pipeline 中的每个 DataNode 写完一个 block 后会进行 ack 验证，以确保所有 DataNode 中的 block 完整。上传完毕后，还会对这些 block 进行检验和验证，进一步确保每个 block 的完整性。

在读取时，当 DataNode 读取 block 时，会计算 checksum。如果计算后的 checksum 与 block 创建时值不一样，说明该 block 已经损坏。Client 会读取其他 DataNode 上的 block，NameNode 标记该块已经损坏，然后复制 block 达到预期设置的文件备份数。

#### CAS 算法原理，底层如何实现的 CAS 原子性
CAS（Compare and Swap，比较并交换）操作的原子性是通过硬件层面的支持来保证的。在现代的 CPU 架构中，通常会提供一些特定的指令来实现原子操作。

以 x86 架构为例，CAS 被翻译为“lock cmpxchg...”指令。其中，“cmpxchg”指令实际上会被拆解为读取、比较和交换三个过程。为了确保这三个过程的原子性，会使用“lock”前缀来锁住缓存行或者总线，保证排他性，使得在做比较和交换的时候不会被其他线程或处理器中断。

在 Java 中，CAS 操作通常由 `unsafe` 类提供支持，`unsafe` 类中的相关方法（如 `compareAndSwapInt` 等）是通过本地（native）方法来访问底层的硬件指令。当 JVM 调用这些本地方法时，会触发相应的硬件指令来执行 CAS 操作，从而保证了原子性。

具体来说，当一个线程执行 CAS 操作时，它首先会读取内存中的值，然后将该值与预期值进行比较，如果相等，则将新值写入内存。在这个过程中，由于有硬件指令的原子性保证，其他线程无法在中间插入或干扰这个操作。如果在比较时发现内存中的值已经被其他线程修改，那么 CAS 操作就会失败，线程可以选择重试或者采取其他策略。

#### 如何在一个 3 G 物理机下部署两个 2 G 的 Java 服务
1. 重新评估和优化服务：
    - 深入分析两个 Java 服务的代码，查找可能的内存泄漏点，并进行修复。
    - 检查数据结构和算法，看是否可以使用更节省内存的数据结构或优化算法，减少内存占用。
2. 服务迁移：
    - 将其中一个或两个服务迁移到具有更多内存资源的服务器上。
    - 考虑使用云服务，根据实际需求灵活调整服务器的内存配置。
3. 水平扩展：
    - 如果服务的业务逻辑允许，可以将服务拆分成多个实例，每个实例处理一部分工作负载，从而降低单个服务的内存需求。
4. 借助缓存服务：
    - 对于一些常用但占用大量内存的数据，可以使用专门的缓存服务（如 Redis）来存储，减少 Java 服务本身的内存消耗。
5. 调整服务架构：
    - 重新设计服务架构，例如采用微服务架构，将功能进一步细分，使得每个服务的内存需求降低。
6. 内存共享：
    - 如果两个服务之间有一些可以共享的内存数据，可以通过共享内存的方式来减少总的内存占用。
7. 分批处理：
    - 如果服务的某些操作可以分批进行，而不是一次性处理大量数据，从而减少瞬时的内存需求。
8. 通过虚拟内存的方式：在 Java 中通常不直接通过设置来使用虚拟内存。虚拟内存是由操作系统管理的。但一般来说，依靠虚拟内存不是一个好的解决方案，因为虚拟内存的性能通常比物理内存差很多，会导致服务的性能严重下降。