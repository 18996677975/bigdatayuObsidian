准确预估一段代码的运行时间：
1. **确定运行平台**，包括硬件配置、编程语言、系统环境等，这些因素都会影响代码的运行效率。
2. **评估各种计算操作所需的运行时间**，例如加法操作 `+` 需要 1 ns ，乘法操作 `*` 需要 10 ns ，打印操作 `print()` 需要 5 ns 等。
3. **统计代码中所有的计算操作**，并将所有操作的执行时间求和，从而得到运行时间。
**统计算法的运行时间既不合理也不现实**。首先，我们不希望将预估时间和运行平台绑定，因为算法需要在各种不同的平台上运行。其次，我们很难获知每种操作的运行时间，这给预估过程带来了极大的难度。

## 2.3.1 统计时间增长趋势
**算法运行时间随着数据量变大时的增长趋势**。
``` python
# 算法 A 的时间复杂度：常数阶
def algorithm_A(n: int):
    print(0)
# 算法 B 的时间复杂度：线性阶
def algorithm_B(n: int):
    for _ in range(n):
        print(0)
# 算法 C 的时间复杂度：常数阶
def algorithm_C(n: int):
    for _ in range(1000000):
        print(0)
```
- 算法 `A` 只有 1 个打印操作，算法运行时间不随着 𝑛 增大而增长。我们称此算法的时间复杂度为“常数阶”。
- 算法 `B` 中的打印操作需要循环 𝑛 次，算法运行时间随着 𝑛 增大呈线性增长。此算法的时间复杂度被称为“线性阶”。
- 算法 `C` 中的打印操作需要循环 1000000 次，虽然运行时间很长，但它与输入数据大小 𝑛 无关。因此 `C` 的时间复杂度和 `A` 相同，仍为“常数阶”。
![[Pasted image 20240602114039.png]]
相较于直接统计算法的运行时间，时间复杂度分析的特点：
* **时间复杂度能够有效评估算法效率**。
* **时间复杂度的推算方法更简便**。
* **时间复杂度也存在一定的局限性**。

## 2.3.2 函数渐近上界
不重要
我们将线性阶的时间复杂度记为 𝑂(𝑛) ，这个数学符号称为大 𝑂 记号（big-𝑂 notation），表示函数 𝑇(𝑛) 的渐近上界（asymptotic upper bound）。

## 2.3.3 推算方法
不重要
### 1. 第一步 统计操作数量
1. **忽略 𝑇(𝑛) 中的常数项**。
2. **省略所有系数**。
3. **循环嵌套时使用乘法**。

### 2. 第二步判断渐近上界
**时间复杂度由 𝑇(𝑛) 中最高阶的项来决定**。

| 操作数量 𝑇(𝑛)      | 时间复杂度 𝑂(𝑓(𝑛)) |
| ---------------- | ---------------- |
| 100000           | 𝑂(1)            |
| 3𝑛+2            | 𝑂(𝑛)           |
| 2𝑛2+3𝑛+2       | 𝑂(𝑛2)          |
| 𝑛3+10000𝑛2     | 𝑂(𝑛3)          |
| 2𝑛+10000𝑛10000 | 𝑂(2𝑛)          |

## 2.3.4 常见类型
设输入数据大小为 𝑛 ，常见的时间复杂度类型（按照从低到高的顺序排列）。
![[Pasted image 20240602114649.png]]
![[Pasted image 20240602114712.png]]

## 2.3.5 最差、最佳、平均时间复杂度
**算法的时间效率往往不是固定的，而是与输入数据的分布有关**。
- 当 `nums = [?, ?, ..., 1]` ，即当末尾元素是 1 时，需要完整遍历数组，**达到最差时间复杂度 𝑂(𝑛)** 。
- 当 `nums = [1, ?, ?, ...]` ，即当首个元素为 1 时，无论数组多长都不需要继续遍历，**达到最佳时间复杂度 Ω(1)** 。

实际中很少使用最佳时间复杂度，因为通常只有在很小概率下才能达到，可能会带来一定的误导性。**而最差时间复杂度更为实用，因为它给出了一个效率安全值**，让我们可以放心地使用算法。